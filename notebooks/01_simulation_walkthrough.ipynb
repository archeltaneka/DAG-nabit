{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 1: The Simulation Engine ðŸ­\n",
    "\n",
    "**Author**: Your Name  \n",
    "**Project**: CausalCommerce - Business Strategy Simulator  \n",
    "**Goal**: Demonstrate understanding of causal inference through synthetic data generation\n",
    "\n",
    "---\n",
    "\n",
    "## What Makes This Special?\n",
    "\n",
    "Most portfolio projects use **static datasets** from Kaggle. This project builds a **dynamic simulation** that:\n",
    "\n",
    "1. **Generates realistic customer populations** with hidden causal structures\n",
    "2. **Simulates business interventions** (discounts, ads, etc.)\n",
    "3. **Introduces realistic bias** that breaks naive analysis\n",
    "4. **Provides ground truth** so we can validate our causal inference methods\n",
    "\n",
    "This approach demonstrates **deep understanding** of:\n",
    "- Causal DAGs (Directed Acyclic Graphs)\n",
    "- Selection bias and confounding\n",
    "- Treatment effect heterogeneity\n",
    "- Real business constraints\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from data.generators import (\n",
    "    SimulationConfig,\n",
    "    generate_customer_data,\n",
    "    BehaviorSimulator,\n",
    "    simulate_scenario,\n",
    "    SEGMENT_PARAMS\n",
    ")\n",
    "\n",
    "# Plotting setup\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.precision', 3)\n",
    "\n",
    "print(\"âœ… Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Understanding the Customer Segments\n",
    "\n",
    "Our simulation models 4 distinct customer types (inspired by real uplift modeling research):\n",
    "\n",
    "| Segment | Base Purchase Rate | Discount Sensitivity | Key Insight |\n",
    "|---------|-------------------|---------------------|-------------|\n",
    "| **Loyalists** | 80% | +5% | Buy anyway - discount wastes money |\n",
    "| **Persuadables** | 30% | +40% | **TARGET THESE!** High ROI |\n",
    "| **Sleeping Dogs** | 15% | -10% | Discounts HURT conversion |\n",
    "| **Lost Causes** | 5% | +2% | Won't buy even with discount |\n",
    "\n",
    "**The Challenge**: In real business, we can't observe segments directly. We only see:\n",
    "- Observable features (age, activity, past purchases)\n",
    "- Outcomes (did they buy?)\n",
    "\n",
    "**Goal**: Use causal inference to identify Persuadables without wasting money on others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize segment parameters\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Purchase probabilities\n",
    "segments = list(SEGMENT_PARAMS.keys())\n",
    "base_probs = [SEGMENT_PARAMS[s].base_purchase_prob for s in segments]\n",
    "discount_effects = [SEGMENT_PARAMS[s].discount_sensitivity for s in segments]\n",
    "\n",
    "x = np.arange(len(segments))\n",
    "width = 0.35\n",
    "\n",
    "axes[0].bar(x - width/2, base_probs, width, label='Base Purchase Prob', alpha=0.8)\n",
    "axes[0].bar(x + width/2, discount_effects, width, label='Discount Effect', alpha=0.8)\n",
    "axes[0].set_xlabel('Customer Segment')\n",
    "axes[0].set_ylabel('Probability')\n",
    "axes[0].set_title('Purchase Behavior by Segment')\n",
    "axes[0].set_xticks(x)\n",
    "axes[0].set_xticklabels(segments, rotation=45)\n",
    "axes[0].legend()\n",
    "axes[0].axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
    "\n",
    "# Plot 2: Expected lift from 20% discount\n",
    "discount_amount = 0.20\n",
    "expected_lifts = [SEGMENT_PARAMS[s].discount_sensitivity * discount_amount * 100 for s in segments]\n",
    "colors = ['green' if lift > 0 else 'red' for lift in expected_lifts]\n",
    "\n",
    "axes[1].bar(segments, expected_lifts, color=colors, alpha=0.7)\n",
    "axes[1].set_xlabel('Customer Segment')\n",
    "axes[1].set_ylabel('Expected Lift (%)')\n",
    "axes[1].set_title('Expected Conversion Lift from 20% Discount')\n",
    "axes[1].set_xticklabels(segments, rotation=45)\n",
    "axes[1].axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ’¡ Key Insight: 'Sleeping Dogs' have NEGATIVE lift - discounts hurt conversion!\")\n",
    "print(\"   This is realistic: some customers see discounts as 'cheap' or spammy.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Generate Customer Population\n",
    "\n",
    "Let's create a synthetic population of 10,000 customers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure simulation\n",
    "config = SimulationConfig(\n",
    "    n_customers=10000,\n",
    "    random_seed=42\n",
    ")\n",
    "\n",
    "# Generate customers\n",
    "customers = generate_customer_data(config)\n",
    "\n",
    "print(f\"Generated {len(customers):,} customers\\n\")\n",
    "print(\"Segment distribution:\")\n",
    "print(customers['segment'].value_counts(normalize=True).sort_values(ascending=False))\n",
    "print(\"\\nFirst few customers:\")\n",
    "customers.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Examine Observable Features\n",
    "\n",
    "These are features we CAN measure in real business (like from a CRM system)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observable_features = [\n",
    "    'age', 'tenure_months', 'activity_score',\n",
    "    'prev_purchases', 'account_value', 'email_engagement_rate'\n",
    "]\n",
    "\n",
    "print(\"Observable Features Summary:\\n\")\n",
    "print(customers[observable_features].describe())\n",
    "\n",
    "# Visualize feature distributions by segment\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, feature in enumerate(observable_features):\n",
    "    for segment in customers['segment'].unique():\n",
    "        data = customers[customers['segment'] == segment][feature]\n",
    "        axes[i].hist(data, alpha=0.5, label=segment, bins=30)\n",
    "    axes[i].set_title(feature)\n",
    "    axes[i].set_xlabel('Value')\n",
    "    axes[i].set_ylabel('Frequency')\n",
    "    if i == 0:\n",
    "        axes[i].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ’¡ Notice: Features are CORRELATED with segment but overlapping.\")\n",
    "print(\"   This is realistic - we can't perfectly identify segments from observables.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Examine Hidden Propensities (Ground Truth)\n",
    "\n",
    "In real life, we NEVER observe these. But in our simulation, we know them!\n",
    "\n",
    "This allows us to:\n",
    "1. Validate our causal inference methods\n",
    "2. Calculate the TRUE treatment effect\n",
    "3. Measure how much money naive analysis wastes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare true propensities by segment\n",
    "propensity_cols = [\n",
    "    'base_purchase_propensity',\n",
    "    'discount_effect',\n",
    "    'churn_propensity',\n",
    "    'avg_purchase_value'\n",
    "]\n",
    "\n",
    "print(\"Ground Truth: True Propensities by Segment\\n\")\n",
    "ground_truth = customers.groupby('segment')[propensity_cols].mean()\n",
    "print(ground_truth.round(3))\n",
    "\n",
    "# Visualize the key insight: discount_effect\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(data=customers, x='segment', y='discount_effect')\n",
    "plt.title('Distribution of TRUE Discount Effects by Segment', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Customer Segment')\n",
    "plt.ylabel('Discount Effect (% increase in purchase probability)')\n",
    "plt.axhline(y=0, color='red', linestyle='--', linewidth=2, label='Zero effect')\n",
    "plt.legend()\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸŽ¯ This is what we're trying to ESTIMATE with causal inference!\")\n",
    "print(\"   Challenge: We only observe outcomes, not these propensities.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Simulate Business Scenarios\n",
    "\n",
    "Now let's simulate what happens when we run marketing campaigns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Scenario A: Perfect Randomized A/B Test\n",
    "\n",
    "This is the **gold standard** - random treatment assignment with no bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate randomized experiment\n",
    "simulator = BehaviorSimulator(customers, config)\n",
    "experiment_random = simulator.simulate_experiment(\n",
    "    treatment_assignment='random',\n",
    "    treatment_probability=0.5,\n",
    "    discount_amount=0.20\n",
    ")\n",
    "\n",
    "print(\"Randomized A/B Test Results\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Check balance\n",
    "print(\"\\nTreatment Balance:\")\n",
    "print(experiment_random['treated'].value_counts())\n",
    "\n",
    "# Naive analysis (what most people do)\n",
    "print(\"\\nNaive Comparison (Simple Average):\")\n",
    "naive_results = experiment_random.groupby('treated').agg({\n",
    "    'purchased': ['mean', 'sum'],\n",
    "    'revenue': 'mean',\n",
    "    'churned': 'mean'\n",
    "})\n",
    "print(naive_results)\n",
    "\n",
    "control_conv = naive_results.loc[0, ('purchased', 'mean')]\n",
    "treated_conv = naive_results.loc[1, ('purchased', 'mean')]\n",
    "naive_lift = (treated_conv - control_conv) / control_conv * 100\n",
    "\n",
    "print(f\"\\nðŸ“Š Naive Lift: {naive_lift:.2f}%\")\n",
    "print(\"\\nâœ… In a randomized experiment, naive analysis is CORRECT!\")\n",
    "print(\"   No confounding, so simple comparison gives true causal effect.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Scenario B: Biased Targeting (The Real World)\n",
    "\n",
    "In reality, treatments are NOT randomly assigned. Marketing teams target \"engaged\" customers.\n",
    "\n",
    "**This creates confounding**: Active customers buy more anyway, even without treatment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate biased targeting\n",
    "experiment_biased = simulator.simulate_experiment(\n",
    "    treatment_assignment='biased_activity',\n",
    "    discount_amount=0.20\n",
    ")\n",
    "\n",
    "print(\"Biased Targeting Results (Marketing targets active customers)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Show the bias in treatment assignment\n",
    "print(\"\\nTreatment Assignment by Activity Level:\")\n",
    "activity_bins = pd.cut(experiment_biased['activity_score'], bins=5, labels=['Very Low', 'Low', 'Medium', 'High', 'Very High'])\n",
    "treatment_by_activity = experiment_biased.groupby(activity_bins)['treated'].agg(['mean', 'count'])\n",
    "treatment_by_activity.columns = ['Treatment Rate', 'Count']\n",
    "print(treatment_by_activity)\n",
    "\n",
    "print(\"\\nâš ï¸  BIAS DETECTED: Higher activity customers get more treatment!\")\n",
    "\n",
    "# Naive analysis (BIASED!)\n",
    "print(\"\\nNaive Comparison:\")\n",
    "naive_biased = experiment_biased.groupby('treated').agg({\n",
    "    'purchased': ['mean', 'sum'],\n",
    "    'revenue': 'mean'\n",
    "})\n",
    "print(naive_biased)\n",
    "\n",
    "control_conv_b = naive_biased.loc[0, ('purchased', 'mean')]\n",
    "treated_conv_b = naive_biased.loc[1, ('purchased', 'mean')]\n",
    "naive_lift_biased = (treated_conv_b - control_conv_b) / control_conv_b * 100\n",
    "\n",
    "print(f\"\\nðŸ“Š Naive Lift: {naive_lift_biased:.2f}%\")\n",
    "print(f\"ðŸ“Š True Lift (from random experiment): {naive_lift:.2f}%\")\n",
    "print(f\"\\nâš ï¸  OVERESTIMATE: {naive_lift_biased - naive_lift:.2f} percentage points!\")\n",
    "print(\"\\nðŸš¨ This is why we need causal inference!\")\n",
    "print(\"   The naive analysis attributes natural differences to the treatment.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Visualize the Confounding Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Activity score distribution by treatment\n",
    "axes[0].hist(experiment_biased[experiment_biased['treated']==0]['activity_score'], \n",
    "             alpha=0.6, bins=30, label='Control', color='blue')\n",
    "axes[0].hist(experiment_biased[experiment_biased['treated']==1]['activity_score'], \n",
    "             alpha=0.6, bins=30, label='Treated', color='orange')\n",
    "axes[0].set_xlabel('Activity Score')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].set_title('Confounding: Activity Score by Treatment Group')\n",
    "axes[0].legend()\n",
    "\n",
    "# Plot 2: Purchase rate by activity and treatment\n",
    "activity_deciles = pd.qcut(experiment_biased['activity_score'], q=10, duplicates='drop')\n",
    "purchase_by_activity = experiment_biased.groupby([activity_deciles, 'treated'])['purchased'].mean().unstack()\n",
    "purchase_by_activity.plot(ax=axes[1], marker='o')\n",
    "axes[1].set_xlabel('Activity Decile')\n",
    "axes[1].set_ylabel('Purchase Rate')\n",
    "axes[1].set_title('Purchase Rate by Activity Level and Treatment')\n",
    "axes[1].legend(['Control', 'Treated'])\n",
    "axes[1].set_xticklabels(range(1, len(purchase_by_activity)+1))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ’¡ Notice: The treatment and control groups have DIFFERENT activity distributions.\")\n",
    "print(\"   This confounds our analysis - we're comparing apples to oranges!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Calculate TRUE Treatment Effects (Ground Truth)\n",
    "\n",
    "Since we generated the data, we can calculate the TRUE causal effect!\n",
    "\n",
    "This will be our benchmark when we apply causal inference methods in Module 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate true Individual Treatment Effect (ITE) for each customer\n",
    "discount_amount = 0.20\n",
    "experiment_biased['true_ITE'] = experiment_biased['discount_effect'] * discount_amount\n",
    "\n",
    "# Average Treatment Effect (ATE)\n",
    "true_ATE = experiment_biased['true_ITE'].mean()\n",
    "\n",
    "# Conditional Average Treatment Effect (CATE) by segment\n",
    "true_CATE = experiment_biased.groupby('segment')['true_ITE'].mean().sort_values(ascending=False)\n",
    "\n",
    "print(\"Ground Truth: TRUE Causal Effects\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nAverage Treatment Effect (ATE): {true_ATE*100:.2f}% increase in purchase probability\")\n",
    "print(f\"\\nConditional Average Treatment Effects (CATE) by Segment:\")\n",
    "for segment, effect in true_CATE.items():\n",
    "    print(f\"  {segment:20s}: {effect*100:+6.2f}%\")\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(10, 6))\n",
    "colors = ['green' if x > 0 else 'red' for x in true_CATE.values]\n",
    "plt.barh(true_CATE.index, true_CATE.values * 100, color=colors, alpha=0.7)\n",
    "plt.xlabel('Treatment Effect (% increase in purchase probability)')\n",
    "plt.title('TRUE Treatment Effects by Customer Segment', fontsize=14, fontweight='bold')\n",
    "plt.axvline(x=0, color='black', linestyle='--', linewidth=2)\n",
    "plt.axvline(x=true_ATE*100, color='blue', linestyle='--', linewidth=2, label=f'ATE: {true_ATE*100:.2f}%')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸŽ¯ MODULE 3 GOAL: Estimate these effects from observed data only!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Business Impact Analysis\n",
    "\n",
    "Let's quantify the COST of getting causality wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume we have budget to target 50% of customers\n",
    "budget_pct = 0.50\n",
    "n_to_target = int(len(experiment_biased) * budget_pct)\n",
    "cost_per_customer = 10  # $10 cost to send discount\n",
    "\n",
    "# Strategy 1: Random targeting (naive)\n",
    "random_targets = experiment_biased.sample(n=n_to_target, random_state=42)\n",
    "random_conversions = (random_targets['base_purchase_propensity'] + \n",
    "                     random_targets['true_ITE']).sum()\n",
    "random_cost = n_to_target * cost_per_customer\n",
    "random_roi = (random_conversions * 100 - random_cost) / random_cost  # Assume $100 per purchase\n",
    "\n",
    "# Strategy 2: Target by observable activity (common practice)\n",
    "activity_targets = experiment_biased.nlargest(n_to_target, 'activity_score')\n",
    "activity_conversions = (activity_targets['base_purchase_propensity'] + \n",
    "                       activity_targets['true_ITE']).sum()\n",
    "activity_cost = n_to_target * cost_per_customer\n",
    "activity_roi = (activity_conversions * 100 - activity_cost) / activity_cost\n",
    "\n",
    "# Strategy 3: OPTIMAL - Target by true treatment effect (what causal inference aims for)\n",
    "optimal_targets = experiment_biased.nlargest(n_to_target, 'true_ITE')\n",
    "optimal_conversions = (optimal_targets['base_purchase_propensity'] + \n",
    "                      optimal_targets['true_ITE']).sum()\n",
    "optimal_cost = n_to_target * cost_per_customer\n",
    "optimal_roi = (optimal_conversions * 100 - optimal_cost) / optimal_cost\n",
    "\n",
    "# Summary\n",
    "print(\"Business Impact: ROI Comparison\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Budget: ${n_to_target * cost_per_customer:,} (target {n_to_target:,} customers)\\n\")\n",
    "\n",
    "results_df = pd.DataFrame({\n",
    "    'Strategy': ['Random', 'Target Active (Naive)', 'Target by ITE (Optimal)'],\n",
    "    'Conversions': [random_conversions, activity_conversions, optimal_conversions],\n",
    "    'ROI': [random_roi, activity_roi, optimal_roi]\n",
    "})\n",
    "results_df['ROI_pct'] = results_df['ROI'] * 100\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "print(f\"\\nðŸ’° Value of Causal Inference:\")\n",
    "print(f\"   Optimal vs Random: {(optimal_roi - random_roi)*100:.1f} percentage points better ROI\")\n",
    "print(f\"   Optimal vs Naive:  {(optimal_roi - activity_roi)*100:.1f} percentage points better ROI\")\n",
    "print(f\"\\nðŸŽ¯ This is what we're building toward in Modules 3-4!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Key Takeaways\n",
    "\n",
    "### What We Built:\n",
    "1. âœ… Realistic customer generator with heterogeneous treatment effects\n",
    "2. âœ… Behavior simulator that creates observational data with confounding\n",
    "3. âœ… Ground truth labels to validate causal inference methods\n",
    "\n",
    "### Why This Matters for Hiring Managers:\n",
    "1. **Demonstrates deep understanding** of causal inference concepts\n",
    "2. **Shows engineering skills** - clean, modular, production-quality code\n",
    "3. **Business-oriented** - framed in terms of ROI and strategy\n",
    "4. **Innovative approach** - goes beyond static Kaggle datasets\n",
    "\n",
    "### Next Steps:\n",
    "- **Module 2**: Experimentation framework (A/B tests, observational studies)\n",
    "- **Module 3**: Causal inference methods (Propensity Score, Double ML, Uplift)\n",
    "- **Module 4**: Business intelligence dashboard and ROI calculator\n",
    "\n",
    "---\n",
    "\n",
    "## 8. Save Data for Next Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save datasets for use in future modules\n",
    "experiment_random.to_csv('../data/experiment_randomized.csv', index=False)\n",
    "experiment_biased.to_csv('../data/experiment_biased.csv', index=False)\n",
    "customers.to_csv('../data/customer_population.csv', index=False)\n",
    "\n",
    "print(\"âœ… Datasets saved!\")\n",
    "print(\"\\nReady to move on to Module 2: Experimentation Framework\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-science",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
